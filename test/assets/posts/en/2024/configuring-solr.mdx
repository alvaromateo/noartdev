<div>
    <section id='post-intro'>
        # Configuring Solr
        <PublishDate year={2024} month={10} day={26}/>
        <Tags list={['development', 'solr', 'search']}/>
        <Summary>
            In this post I will show how to configure 
            <Link href='https://solr.apache.org/'>Solr</Link> to use it as search engine for a blog
            website.
        </Summary>
        <ContentTable/>
    </section>
    <section id='post-content'>

        In this previous <Link href='/blog/installing-solr'>article</Link> I explained how to install 
        <Link href='https://solr.apache.org/'>Solr</Link> using 
        <Link href='https://docs.docker.com/compose/gettingstarted/'>Docker Compose</Link> to be 
        able to use it as search engine for a blog website. Then I explained which features I wanted 
        for the search and I decided on a <Link href='/blog/planning-search'>general plan</Link> to get 
        there. Now I'm going to explain how I configured Solr to power the search of 
        <Link href='/home'>this website</Link>.

        Solr has several files that are used to configure its functionality. They are located 
        inside the Solr home directory, which inside the Docker instance is located at 
        */var/solr/data* and has the following structure.

        <CodeBlock language='text'>
{`/var/solr/data/
    solr.xml
    core_name/
        core.properties
        conf/
            solrconfig.xml
            managed-schema.xml
        data/
`}
        </CodeBlock>

        If you followed the article I did on <Link href='/blog/installing-solr'>how to install Solr</Link> 
        you should already have the *conf/* files ready to be edited, so let's dive in!

        ## solr.xml

        This file goes directly inside the Solr home directory and specifies the configuration 
        for the server instance. I have no need for this file as all the defaults are 
        enough (for now at least). The file will be created automatically by Solr.

        ## Core configuration

        A core in Solr is the same as an index. It contains all the necessary files for indexing the collection, 
        a transaction log, and the configuration for the index. One can configure different 
        cores to create indexes that search through different types of documents or over the same 
        documents but in different ways.
        
        I only need a core/index for searching through all the blog posts. 
        The core has 2 main configuration files, located in the structure specified above.

        ### core.properties

        The core.properties file is used to configure simple core properties. It is created automatically 
        during Solr's <Link href='/blog/installing-solr'>installation</Link>.

        ### solrconfig.xml

        To configure the *solrconfig.xml* I took the default configuration file provided by Solr.
        Inside the docker container of Solr it can be found at */opt/solr/server/solr/configsets/_default*. 
        The file is very well documented, so it's easy to read and understand what each part does. 
        Most of the default configuration can be left as is. I will mention here the parts that I decided 
        to change.

        The <CodeBlock language='xml' inline>\<updateHandler\></CodeBlock> manages how the index updates are 
        done and it may affect performance. When I add a document to Solr, it is not searchable until it 
        is commited. While Solr makes its writes it can slow down, which might affect performance. Too 
        many commits and it will be slower, but too few and it might not find the latest data. In the case 
        of this blog, the searchable data are the posts - these are known and published at build time, so 
        I don't have to worry about commiting too often.
        
        Another distinction is that between *hard commit* and *soft commit*. A hard commit makes sure that 
        the index files have been flushed to stable storage and opens a new transaction log (rolling window 
        of updates since the last hard commit that help with data recovery if there is any crash and updates 
        that have not been commited). A soft commit is faster because it only makes the index changes visible 
        without writing any file. In the case of just a commit after a blog post has been publish I can deal 
        with only hard commits.
        
        To takes care of these use cases, I deleted the 
        <CodeBlock language='xml' inline>\<autoSoftCommit\></CodeBlock> configuration and left the following 
        options for committing automatically:

        <CodeBlock language='xml'>
{`<autoCommit>
    <maxTime>\${solr.autoCommit.maxTime:15000}</maxTime>
    <openSearcher>false</openSearcher>
</autoCommit>
`}
        </CodeBlock>

        #### Cache warming

        Solr uses caches to improve performance. These are cleared after a commit and they need to be 
        repopulated before being useful. To do this, caches can be "warmed" by populating the cache with 
        values from a previous cache. All settings related to caching can be controlled in the 
        <CodeBlock language='xml' inline>\<query\></CodeBlock> section. I left it as is by default, as the 
        caches size is set to 512 documents, which is far larger than the posts I have (for now...).

        There is the possibility to define queries that are run when new Index Searchers are created. 
        These are the algorithms in charge of doing the actual search through an index whenever a query is 
        received. Running these queries means that the caches of each Index Searcher will be pre-populated 
        so that the response time of the first user queries hopefully gets a hit in the cache and is faster.

        #### Request handlers

        The next sections are the request handlers that respond to the queries/updates sent to Solr. 
        The request handlers can use defined <Link href='#search_components'>search components</Link> to perform extra 
        actions like spell checking. I removed all the request handlers and search components to start with 
        a blank slate.

        I will define 2 request handlers to take care of the *query* and *suggest* endpoints. The *update* 
        endpoint doesn't need to be defined as there's already an 
        <Link href='https://solr.apache.org/guide/solr/latest/configuration-guide/implicit-requesthandlers.html#update-handlers'>implicit handler</Link> 
        provided by Solr that takes care of JSON updates.

        <CodeBlock language='xml'>
{`<requestHandler name="/query" class="solr.SearchHandler">
    <lst name="defaults">
        <str name="defType">lucene</str>
        <str name="echoParams">explicit</str>
        <str name="wt">json</str>
        <str name="indent">true</str>
    </lst>
</requestHandler>
`}
        </CodeBlock>

        This is the *query* request handler. Basically, you just need to give a name to the handler, specify
        the *SearchHandler* class and then you can define the optional 
        <Link href='https://solr.apache.org/guide/solr/latest/configuration-guide/requesthandlers-searchcomponents.html#defaults-appends-and-invariants'>defaults, appends and invariants</Link> 
        lists of properties. In my case I want to always echo the parameters passed to a query in the response, 
        and I want the response to be in JSON and indented.

        The first property - *defType* - tells Solr which query parser to use when processing the queries it receives. 
        <Link href='https://solr.apache.org/guide/solr/latest/query-guide/edismax-query-parser.html'>eDisMax</Link> 
        processes simple phrases and searches for individual terms accross the document fields, weighing each 
        field differently depending on its significance (of course, all this is to be configured first). It's 
        a similar type of search than the one you can see using <Link href='https://www.google.com/'>Google</Link>.

        <CodeBlock language='xml'>
{`<requestHandler name="/suggest" class="solr.SearchHandler">
    <lst name="defaults">
        <str name="defType">edismax</str>
        <str name="echoParams">explicit</str>
        <str name="wt">json</str>
        <str name="indent">true</str>
        <int name="rows">5</int>
    </lst>
</requestHandler>
`}
        </CodeBlock>

        For the *suggest* request handler I use exactly the same defaults as for the *query* handler, but I add 
        also as default that the response should only contain 5 rows of results and I change the query parser type. 
        <Link href='https://solr.apache.org/guide/solr/latest/query-guide/standard-query-parser.html'>Lucene</Link> 
        is Solr's *Standard Query Parser*. It is less tolerant of syntax error, but to find exact matches of 
        pages that contain a word in any (sub)title it is what I want. If the user has made some syntax mistake, 
        finishes typing and sends the search request, it will be handled by the *query* endpoint which 
        uses *eDisMax*, so the results will be tolerant of the mistake and display in the results what the user 
        probably meant to type.

        As you can see, there is some repetition in the 2 query handlers. There's a tag called *initParams* that 
        is useful for situations like this one. 
        <Link href='https://solr.apache.org/guide/solr/latest/configuration-guide/initparams.html'>InitParams</Link> 
        allows to define request handler parameters outside of the handler and then they can be shared by the 
        specified handlers. My configuration using *initParams* would end up as:

        <CodeBlock language='xml'>
{`<requestHandler name="/suggest" class="solr.SearchHandler">
    <lst name="defaults">
        <str name="defType">lucene</str>
        <int name="rows">5</int>
    </lst>
</requestHandler>

<requestHandler name="/query" class="solr.SearchHandler">
    <lst name="defaults">
        <str name="defType">edismax</str>
    </lst>
</requestHandler>

<initParams path="/query,/suggest">
    <lst name="defaults">
        <str name="echoParams">explicit</str>
        <str name="wt">json</str>
        <str name="indent">true</str>
        <str name="fl">url,title,date,tags</str>
    </lst>
</initParams>
`}
        </CodeBlock>

        I added another default for both request handlers. The field *fl* sets which fields from the document 
        are returned in the response (they need to be marked as *stored* or as *docValues*, which I will explain 
        later in the <Link href='#managed-schema.xml'>managed-schema.xml</Link> section).

        #### Search components

        *Search components* consist of search features (i.e. highlighting or faceting). 

        maybe spell check for the /query endpoint (?) -> probably not needed and one less problem
        highlighting and faceting for the /query endpoint

        #### Update Request Processors

        Last but not least there are the *Update Request Processors (URP)* - the search components equivalent 
        but for update requests. Every update runs through a chain of URPs.

        TODO: configure update request processors, request handlers and search components

        ### managed-schema.xml

        This file declares all the document fields and how they are analyzed. I decided to declare the 
        following types for each post/document that I will send to Solr to index.
        - url
        - title
        - date
        - tags
        - summary
        - sections (for the subtitles)
        - text
        - code

        todo

        ## Conclusion

        TODO

        Until next time!

    </section>
</div>